
=== Microservices 
Think of Microservices as a variant of service-oriented architecture that structures 
the entire application as a collection of loosely coupled services.

Typically, microservices are broken up with the intent to solve a specific business need or
enable a business capability. 

==== Benefits 
- freedom to create, manage, and deploy the individual components without compromising the 
integrity of the entire application.
- Enables fault tolerance and dfault isolation
- language independence
- use the right language at the hand 
- Not pigeonhold to a single language just because the rest of the application uses it.
- avoid bottleneck of scaling
- Can be deployed independently
- Risk of rolling the change back is easy

==== Continuous Delivery Deployment Types 
Blue Green deployment or Red Black Deployment  - Switch between servers and ensure at least one server
is alive at all time

A/B Deployment - Routing percentage of traffic on each while deploying   

Rollback deployment - slowly starting from 1-2 servers instead of going for all simultaneous deployment 
to ensure the successfully deployment  

Canary Deployment - release particular feature for certain set of users (Based on region or first name 
or age range)

Minimum In-Service deployment - make a policy that minimum numbers of server stay alive while 
deployment is in progress.

REFERENCE : https://caylent.com/docker-continuous-delivery-deployment-types/

Building microservices forces you to think about an applications in a more modular way.

For Greenfield opportunities there is a initial learning curve if we use microservice architecture and 
which would pay greater advantage on a long term.

==== The Twelve factor App
Applicable for all cloud and container application

I. Codebase
One codebase tracked in revision control, many deploys
GIT,DOCKER

II. Dependencies
Explicitly declare and isolate dependencies
MAVEN,NPM,GRADLE
In kubernetes create a POD that can have all your dependencies. Commonly seen in kubernetes world - SideCar Pattern

III. Config
Store config in the environment not part of codebase 
   
Small footprint(small organisations ): Different namespaces with different credentials dev, staging and 
production Large footprint(larger organisations):Unique kubernetes installation for dev, stage, and production

Applicaton config for kubernetes is ConfigMap and Secrets 

IV. Backing services
Treat backing services as attached resources
MYSQL, SMTP EMAIL

V. Build, release, run
Strictly separate build and run stages
JENKINS

VI. Processes
Execute the app as one or more stateless processes
Sticky session needs to be revisited and re-implemented

VII. Port binding
Export services via port binding

VIII. Concurrency
Scale out via the process model

IX. Disposability
Maximize robustness with fast startup and graceful shutdown
Quick Application startup and shutdown 

X. Dev/prod parity
Keep development, staging, and production as similar as possible

XI. Logs
Treat logs as event streams
In kubernetes, common to use a log router (Beats/Fluentd) to save the logs to a service (ElasticSearch/Splunk)

XII. Admin processes
Run admin/management tasks as one-off processes


All 12 Rules are basically categorised in 3 parts.
- Microservices Building Block 
    - Codebase 
    - Dependencies 
    - Admin Processes
    - Dev/Staging/Production Parity 

- Deployment Patterns 
    - Application Configurations 
    - Build, release, and run 
    - Processes
    - Port Binding 

- Runtime Pattersn 
    - Backing around services 
    - Features and Concurrency 
    - Disposability
    - Log Management 


REFERENCE : https://12factor.net/

REFERENCE : https://martinfowler.com/microservices/


===== Deploying our application to Kubernetes

We're ready to deploy our application to Kubernetes, but let's take a look at our assets.

====== Goals:
1. View our sample application and containers
2. Take a look at our deployment file 
3. Take a look at our alternate deployment file
4. Deploy our application into kubernetes and verify we can see our API's working.

======= Goal 1
View the sample application here: 

======= Goal 2
To view the deployment file, take a look at wishlist-deployment.yaml

======= Goal 3
To see another way to run the microservices, take a look at wishlist-deployment-alernate.yaml

======= Goal 4
To run the microservice described in goal #1, from the current directory, run:

    kubectl create -f wishlist-deployment.yaml


To verify that the deployment is online:
    
    kubectl get deployments


To verify that the replica sets are running:
    
    kubectl get rs


To verify that the pods are running:
    
    kubectl get pods


To see the services:
    
    kubectl get services


To interact with your API's in the minikube environment:
    
    minikube service wishlist-service


# Wishlist deployment yaml - kindly refer "wishlist-deployment.yaml"

======= REFERENCE 
Project Source code Link : 
https://github.com/bhopals/wishlist

Docker Hub Image Links : 
https://hub.docker.com/r/karthequian/wishlist
https://hub.docker.com/r/karthequian/wishlist-auth
https://hub.docker.com/r/karthequian/wishlist-catalog



======  Alternate Universe! - Microservices deployment 
In the above examle we had Single Deployment and Single Service that contains all 3 
part of the application.

Here we would breakup the all three projects into three deployment files. 
(More of a microservices way)
So its easy to distribute the ownership of the deployments to different teams.


# Wishlist deployment yaml - kindly refer "wishlist-deployment-alternate.yaml"

==== Adding Context with Config Maps 

# Configmaps

- Config information should live outside the app 
- Implements using ConfigMaps in kubernetes 
- Can be passed as an environment variable ( For the small set of the data )
- Can be used as a volume mount ( For the larger set of the data )

Configuration information should live outside of the application. How can we do this in Kubernetes?


## Goals
1. Create a configmap that can be referenced by the application via env variables
2. Create a configmap that can be referenced by the application via a volume mounted file


## Goal 1 - Add configuration data in ENVIRONEMNT variable of the container 
Create the deployment by running 

    kubectl apply -f wishlist-deployment-configmap-simple.yaml


Exec into the auth container in the wishlist pod with a command like:
    
    kubectl exec -it wishlist-<podid> -c auth bash


To look find your env variable run:

    env | grep LOG_LEVEL


# Configmap (Step 1: Create it) - kindly refer "wishlist-deployment-configmap-simple.yaml"

## Goal 2  - Mount configuration as a data volume in container 
Create the deployment by running 

    kubectl apply -f wishlist-deployment-configmap-advanced.yaml

Exec into the auth container in the wishlist pod with a command like:
`kubectl exec -it wishlist-<podid> -c auth bash`
kubectl exec -it <wishlist-podid> /bin/bash 

To look find your env variable run:
`cat /var/lib/wishlist/log.properties`

# Configmap (Step 1: Create it) - kindly refer "wishlist-deployment-configmap-advanced.yaml"




==== Secrets 


## Goals
1. Create a secret that can be referenced by the application via env variables
2. Create a secret that can be referenced by the application via a volume mounted file

## Goal 1 - Add Connection String in ENVIRONMENT VARIABLE 
Create the deployment by running 
`kubectl apply -f wishlist-deployment-secret.yaml`

kubectl exec -it <deployment-id> /bin/bash 
kubectl exec -it <deplpyment-id> -c <container-name> bash 

To look find your env variable run:
`env | grep MYSQL_CONNECTION_STRING`

To decode the connection string which is already encoded
    echo <connection-string> | base64 --decode 

To encode the connection string which is already decoded
    echo <connection-string> | base64 --encode 


## Goal 2 - Mounting Volume of Connection String 

Create the deployment by running 
`kubectl apply -f wishlist-deployment-secret.yaml`

kubectl exec -it <deployment-id> /bin/bash 
kubectl exec -it <deplpyment-id> -c <container-name> bash 

Both commands are same 
-c = Container name param 

To look find your env variable run:
`cat /etc/mysql/connection-string`

# Secret Key YAML File - kindly refer "wishlist-deployment-secret.yaml"


==== Liveness Probes

## Goals: 
1) Understand liveness probes in Kubernetes

## Goal 1
Liveness Probes are often used in deployments with many containers. They help with startup and container running states (https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes )

To run our example:
`kubectl apply -f wishlist-deployment-liveness.yaml`

To see if our probes are running:
`kubectl describe <pod_name>`


=== Advance Topics 


====  Helm

## Goals:
1. Understand what Helm is.
2. Understand what Helm is with respect to our application
2. Run the helm chart for our application


## Goal 1
Helm (https://helm.sh) is a package manager for Kubernetes. Kubernetes is all about yaml files, 
and after a while, when your yaml files grow large, it becomes incredibly painful to debug issues.

This is why people use Helm- to manage complexity in their yaml's. It also provides a way to easily 
update and rollback their kubernetes artifacts. And finally, it's also the most popular place to find 
user generated charts. Think of it like the maven or npm for Kubernetes

## Goal 2
Take a look at the wishlist folder for our deployment and service converted to a helm chart.

## Goal 3

I already have helm installed via the instructions at: https://docs.helm.sh/using_helm/#installing-helm

For reference, I'm running: 
```
helm version
Client: &version.Version{SemVer:"v2.8.2", GitCommit:"a80231648a1473929271764b920a8e346f6de844", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.8.2", GitCommit:"a80231648a1473929271764b920a8e346f6de844", GitTreeState:"clean"}
```

To see existing charts:
`helm ls`

To see the tiller components:
`kubectl get deployments --all-namespaces`

To run our helm chart:
`helm install -n wishlist-chart  -f values.yaml .`

==== Service Proxy with Envoy 
# Playing with ingress controllers

## Goals:
1. Understand what ingress/envoy is.
2. See how envoy fits in...
3. How it can be implemented with Contour and envoy

# Goals 1
What is it?
Services are of 3 types: ClusterIP, NodePort and Loadbalancers. ClusterIP and NodePort are used for applications internal to your infrastructure. For applications that you'd want to expose externally, you'd use a loadbalancer service. This is great, but for every endpoint, you'd end up using another loadbalancer resource from your cloud provider, and costs you a bit more.

Ingress allows you you to route requests to services based on the request host or path, centralizing a number of services into a single entrypoint. So think of it as the central point for 1 entrypoint for multiple requests, where loadbalancer is a 1 entrypoint for a specific host or path.

Ingress information: https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress

# Goals 2
Envoy is a simple service proxy that proxies traffic from one source to another. The goal of envoy is to make networking and observability of your applications more visible.

When all service traffic flow through an Envoy mesh, you can visualize problem areas via consistent observability, tune overall performance or add features like rate limiting in a single spot.

Link: https://www.envoyproxy.io/

It's also common to see it used an ingress controller either by itself, or using another package that extends it- like Heptio Contour (https://github.com/heptio/contour). We'll use contour for our example.

# Goals 3

First, we need to add Contour to our cluster. I'm going to follow the docs (https://github.com/heptio/contour#add-contour-to-your-cluster), and install Contour with:

`kubectl apply -f https://j.hept.io/contour-deployment-rbac`

Then, I'll deploy my application:
`kubectl apply -f wishlist-contour.yaml`

To check the status:
`kubectl get ing`

To get the contour ingress URL, I can run:
`minikube service -n heptio-contour contour --url`

Here "-n" is for namespace falg, and since the heptio service is running in its own namepspace so to 
access that we need namespace+servince name.


You can hit the host:port/wishlist URL to see the wishlist API's working.



==== Metrics with Prometheus

## Goals:
1. Deploy Prometheus
2. See it running!
3. View Kubernetes stats
4. View node information
5. View application metrics

# Prometheus + Kubernetes demo

## Step 1: Deploy
Helm is the easiest way to do this. Check out `https://github.com/kubernetes/charts/tree/master/stable/prometheus`.

We can run `helm install stable/prometheus` to get the stock prometheus server.

In this case, we will run: `helm install stable/prometheus --name prom-demo -f values.yaml` to use our custom yaml.

## Step 2: See it running
Running `minikube service prom-demo-prometheus-server` will bring up the browser with prometheus server running.

## Step 3: Check out Kubernetes stats
Check out `count(kube_pod_container_status_running)` to see our all our pods running.

## Step 4: Check out node information

The node exporter gives you node relative information as well like CPU/disk usage etc.

Run `count(node_cpu{mode="system", instance="192.168.99.100:9100"})` will return the cpu count which should match the number of CPU's in `kubectl describe nodes`

### Step 5: App metrics

1. Run the app: `kubectl apply -f wishlist.yaml`
2. Visit the app after it's deployed: `minikube service wishlist-service`
3. You'll see the  `/metrics` endpoint with go stats
4. Visit the `/products` endpoint 5 times.
5. You'll see a new "product_calls" metric in the dashboard



==== Logging from your application

## Goals
1. Start up EFK stack in minikube
2. Run your application and see the logs in Kibana

## Goal 1
We can start up the EFK (Elastic-Fluentd-Kibana) stack in minikube. Installing these components is a little bit of work, but minikube gives it to us as an addon.

`minikube addons enable efk`

Note, this will take a while, and you probably want to use minikube in a high memory mode.
To increase the memory, you can type:
minikube start --memory 16000 -cpus 4


Once it's up and running, visit and configure Kibana by going to the URL by typing:

`minikube addons open efk`

## Goal 2
Now let's deploy our application by running

`kubectl apply -f wishlist-deployment.yaml`

The app has some logs in it that we can look at in Kibana.


==== Tracing issues with Jaeger

## Goals:
1. Understand what Jaeger does
2. See it running
3. See data flowing in it for the hotrod application

# Goals 1
What is Jaeger?

Jaeger is a distributed tracing analyzer that was released by Uber. It is compatible with the opentracing standard and both opentracing and Jaeger are CNCF projects. If you're new to the space, think of Jaeger as an opensource New Relic

https://github.com/jaegertracing/jaeger-kubernetes

Once deployed, startup Jaeger with `minikube service jaeger-query`

# Goals 2
See it in action!

We can install Jaeger from the github site for their kubernetes project: https://github.com/jaegertracing/jaeger-kubernetes

# Goals 3
Let's deploy our sample application with the command:
`kubectl apply -f jaeger-example.yaml`


=== NEXT 

Service mesh - A dedicated infrastructure layer to handle service to service communication 
    Contour + envoy - Ingress that acts as Service Mesh        

Istio - An open source platform to connect, manage and secure microservices.
Platform on top of Kubernetes.
Application Developers use Istio commands to interact with their cluster instead of kubectl commands.

==== Types of health checks
Kubernetes gives you two types of health checks, and it is important to
understand the differences between the two, and their uses.

===== Readiness
Readiness probes are designed to let Kubernetes know when your app is ready to serve traffic. 
Kubernetes makes sure the readiness probe passes before allowing a service to send traffic to the pod. 
If a readiness probe starts to fail, Kubernetes stops sending traffic to the pod until it passes.

===== Liveness
Liveness probes let Kubernetes know if your app is alive or dead. If you app is alive, then 
Kubernetes leaves it alone. If your app is dead, Kubernetes removes the Pod and starts a new one to 
replace it.




==== kubectl create v/s apply 

These are two very different approaches. 

1.  kubectl create uses Imperative Management. Here You specify what you want to create, delete or replace.

    kubectl apply uses what we call Declarative approach. Here we tell the api How we want the cluster to 
    look like. So even if you've applied changes to a live object, your changes are maintained.

2.  Also, if you run the kubectl create second time, it will throw an error as the deployments and services
    are already created.However, if you run kubectl apply again then it will modify exisiting deployed 
    services and deployments if there is any change in the configuration, else it will remain unchanged 
    and command will be executed without throwing any error.


